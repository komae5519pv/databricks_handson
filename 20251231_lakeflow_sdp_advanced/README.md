## 🔴 上級：高度なデータ整合性とリカバリ設計

**シナリオ**: 現実のデータ運用で頻発する「仕様変更」「データの到着遅延」「マスタ不足」に対する堅牢なパイプラインを構築します

* **Watermark（遅延制御）**: `transaction_date` を基準に水位を管理し、システム負荷を抑えつつ「許容範囲内の遅延データ」のみを正確に救済・破棄する
* **Late-Arriving Dimensions（バックフィル）**: 注文時に不在だったマスタを後から補充（Backfill）し、**マテリアライズド・ビュー（MV）** の再計算機能によって過去のNULLを自動的に正しい名称へ補完する
* **2段構えのSilver層**: ストリーミングによる Fact 確定（Watermarked）と、バッチ的性質を持つ結合・加工（Enriched）を分離し、整合性とパフォーマンスを両立させる

---

## 📂 フォルダ構成とファイル詳細

### 📜 transformations/

* **silver/**<br>
  * `backfill_users.py`: **[上級]** マスタ不足を解消するための救急用CDCフロー。`once=True` で起動時のみ実行
  * `enrich_transactions.py`: **[上級]** Watermarkによるストリーミング制御と、MVによる自動補完を組み合わせた高度な実装
* **gold/**<br>
  * `revenue_analysis.py`: **[上級]** 年齢層セグメントや時間軸、カテゴリ別内訳など、複雑な分析ディメンションを付与した最終ビュー

  [構文の詳細はこちらをご覧ください](https://docs.databricks.com/ldp/developer/python-ref)<br>
  [チュートリアルや参考資料はこちらをご覧ください](https://docs.databricks.com/ldp)<br>
  [Spark Declarative Pipelines Programming Guide](https://spark.apache.org/docs/latest/declarative-pipelines-programming-guide.html#flows)<br>


### 📝 explorations/

ハンズオンの操作用ノートブックです。<br>

* `00_config`: パス設定やディレクトリの初期化
* `01_初期データ生成`: 環境初期化とベースデータの作成（意図的にマスタ不足状態を創出）
* `02_仕様変更(スキーマ進化)`: **[上級]** 新カラム(discount)の追加投入による自動追従の検証
* `03_遅延データ追加(Watermark)`: **[上級]** 正常な遅延データと、古すぎて除外されるデータの境界線を検証
* `04_マスタ補充(Backfill)`: **[上級]** 不在だったマスタを投入し、過去のNULLデータを自動補完（自己修復）
* `99_サンプルデータ全削除`: 学習環境のクリーンアップ（フォルダ・テーブル削除）

---

## 🛠️ 上級ハンズオンのハイライト

1. **Schema Evolutionの自動化**:<br>
ソースに新しい列が増えた際、SDPが自動でテーブル定義を拡張し、即座に分析に回せる俊敏性を確認します。
2. **Watermarkによる「境界線」の可視化**:<br>
「昨日届いたデータは入るが、1ヶ月前のは入らない」という挙動を通じ、ストリーミングエンジンの状態管理を理解します。
3. **マテリアライズド・ビューの「自己修復」**:<br>
バックフィルによってマスタが揃った瞬間、パイプラインが依存関係を自動検知して過去のNULLを埋め直す、Databricksならではの高度な整合性維持を体験します。

---

## 🚀 ハンズオンの進め方

1. `01_初期データ生成` でベースラインを作成。
2. `02_仕様変更` を実行し、パイプラインを再起動してスキーマ進化を確認
3. `03_遅延データ追加` を実行し、Watermarkによって ID:204 は取り込まれ、ID:202 は無視される様子をUIで観察
4. この時点で ID:203 の名前が `null` であることを確認
5. `04_マスタ補充` を実行。パイプラインを「Start」し、**Full Refresh なし** で Gold 層までの NULL が名前に書き換わることを確認！
